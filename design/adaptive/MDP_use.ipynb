{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20efa36e-c57e-4cd6-b4ce-f09589e1f9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import MDP\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a478310b-6900-4e71-94f7-7d673a4f6709",
   "metadata": {},
   "source": [
    "See example in page 31 of `Markov Decision Processes`, by Lodewijk Kallenberg (Leiden)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be40c5d1-e133-4658-9987-d7499b2fa304",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = [0,1]\n",
    "A = [1,2]\n",
    "horizon = 3\n",
    "delta = 1\n",
    "\n",
    "def u_1(a):\n",
    "    if a == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def u_2(a):\n",
    "    if a == 1:\n",
    "        return 2\n",
    "    else:\n",
    "        return 5\n",
    "\n",
    "def p_11(a):\n",
    "    if a == 1:\n",
    "        return 1/2\n",
    "    else:\n",
    "        return 1/4\n",
    "    \n",
    "def p_12(a):\n",
    "    if a == 1:\n",
    "        return 1/2\n",
    "    else:\n",
    "        return 3/4\n",
    "    \n",
    "def p_21(a):\n",
    "    if a == 1:\n",
    "        return 2/3\n",
    "    else:\n",
    "        return 1/3\n",
    "    \n",
    "def p_22(a):\n",
    "    if a == 1:\n",
    "        return 1/3\n",
    "    else:\n",
    "        return 2/3\n",
    "    \n",
    "trans_prob_mat = np.array([\n",
    "    [p_11, p_12],\n",
    "    [p_21, p_22]\n",
    "])\n",
    "\n",
    "reward_vector = np.array([u_1, u_2])\n",
    "\n",
    "trans_probs = [trans_prob_mat]*horizon\n",
    "rewards = [reward_vector]*horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abdf4506-41fc-4c3a-80c8-003aef8e943f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp_example = MDP.MDP(S, A, rewards, trans_probs, horizon, delta)\n",
    "mdp_example.fit_optimal_values(verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36cc81d0-033a-4702-9093-93d3b9efc935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.5       , 12.11111111])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdp_example.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ab40c0c-1af4-4c92-9d38-32cdbfdb5491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.5       , 12.11111111])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([15/2,109/9])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a227b66e-69c6-49ae-8ab8-fe7c5706db95",
   "metadata": {},
   "source": [
    "Coincides with the example of the book. Great! Also the actions that were decided by the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcb38273-0f23-49f5-b43b-b387cf6c82e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{0: 1, 1: 2}, {0: 1, 1: 2}, {0: 2, 1: 2}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdp_example.policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922efce3-fa9e-4029-9a0a-25a2a7af028a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
